{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noriakihanya/text2animation/blob/main/text2animation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ランタイムの設定\n",
        "\n",
        "「ランタイム」→「ランタイムのタイプを変更」→「ハードウェアアクセラレータ」をGPUに変更"
      ],
      "metadata": {
        "id": "VWZE_KOxtYVB"
      },
      "id": "VWZE_KOxtYVB"
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "oRJLO_4Jte7D"
      },
      "id": "oRJLO_4Jte7D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "089eb903",
      "metadata": {
        "cellView": "form",
        "id": "089eb903"
      },
      "outputs": [],
      "source": [
        "#@title Workspace作成\n",
        "import os\n",
        "\n",
        "workspace_vq = \"/content/Workspace/vqgan/steps\"\n",
        "os.makedirs(workspace_vq, exist_ok=True)\n",
        "workspace_im = \"/content/Workspace/vqgan/images\"\n",
        "os.makedirs(workspace_im, exist_ok=True)\n",
        "\n",
        "working_dir = \"/content/Workspace/vqgan\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b1e0e19",
      "metadata": {
        "scrolled": true,
        "cellView": "form",
        "id": "3b1e0e19"
      },
      "outputs": [],
      "source": [
        "#@title　ライブラリのインストール\n",
        "%cd /content/\n",
        "\n",
        "print(\"Downloading CLIP...\")\n",
        "!git clone https://github.com/openai/CLIP                 &> /dev/null\n",
        "\n",
        "print(\"Downloading Python AI libraries...\")\n",
        "!git clone https://github.com/CompVis/taming-transformers &> /dev/null\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning  &> /dev/null\n",
        "!pip install kornia                                       &> /dev/null\n",
        "!pip install einops                                       &> /dev/null\n",
        " \n",
        "print(\"Installing libraries for handling metadata...\")\n",
        "!pip install stegano                                      &> /dev/null\n",
        "!apt install exempi                                       &> /dev/null\n",
        "!pip install python-xmp-toolkit                           &> /dev/null\n",
        "!pip install imgtag                                       &> /dev/null\n",
        "!pip install pillow==7.1.2                                &> /dev/null\n",
        " \n",
        "print(\"Installing Python video creation libraries...\")\n",
        "!pip install imageio-ffmpeg &> /dev/null\n",
        "print(\"Installing Google-API libraries...\")\n",
        "!pip install keybert\n",
        "!pip install git+https://github.com/alainrouillon/py-googletrans@feature/enhance-use-of-direct-api\n",
        "path = f'{working_dir}/steps'\n",
        "!mkdir --parents {path}\n",
        "print(\"Installing text2animation...\")\n",
        "!git clone https://github.com/noriakihanya/text2animation.git\n",
        "print(\"Installation finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8718a997",
      "metadata": {
        "cellView": "form",
        "id": "8718a997"
      },
      "outputs": [],
      "source": [
        "#@title ライブラリのインポート\n",
        "import shutil\n",
        "\n",
        "import argparse\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import ast\n",
        " \n",
        "sys.path.append('/content/taming-transformers')\n",
        "\n",
        "# Some models include transformers, others need explicit pip install\n",
        "try:\n",
        "    import transformers\n",
        "except Exception:\n",
        "    !pip install transformers\n",
        "    import transformers\n",
        "\n",
        "from IPython import display\n",
        "from base64 import b64encode\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from taming.models import cond_transformer, vqgan\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        " \n",
        "from CLIP import clip\n",
        "import kornia.augmentation as K\n",
        "import numpy as np\n",
        "import imageio\n",
        "from PIL import ImageFile, Image\n",
        "from imgtag import ImgTag    # metadata \n",
        "from libxmp import *         # metadata\n",
        "import libxmp                # metadata\n",
        "from stegano import lsb\n",
        "import json\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3c35a6d1",
      "metadata": {
        "cellView": "form",
        "id": "3c35a6d1"
      },
      "outputs": [],
      "source": [
        "#@title 関数定義\n",
        "\n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        " \n",
        " \n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        " \n",
        " \n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        " \n",
        " \n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        " \n",
        "    input = input.view([n * c, 1, h, w])\n",
        " \n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        " \n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        " \n",
        "    input = input.view([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        " \n",
        " \n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        " \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        " \n",
        " \n",
        "replace_grad = ReplaceGrad.apply\n",
        " \n",
        " \n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        " \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        " \n",
        " \n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        " \n",
        " \n",
        "def vector_quantize(x, codebook):\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "    indices = d.argmin(-1)\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "    return replace_grad(x_q, x)\n",
        " \n",
        " \n",
        "class Prompt(nn.Module):\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        " \n",
        "    def forward(self, input):\n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
        "        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        " \n",
        " \n",
        "def parse_prompt(prompt):\n",
        "    vals = prompt.rsplit(':', 2)\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "    return vals[0], float(vals[1]), float(vals[2])\n",
        " \n",
        " \n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.augs = nn.Sequential(\n",
        "            K.RandomHorizontalFlip(p=0.5),\n",
        "            # K.RandomSolarize(0.01, 0.01, p=0.7),\n",
        "            K.RandomSharpness(0.3,p=0.4),\n",
        "            K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'),\n",
        "            K.RandomPerspective(0.2,p=0.4),\n",
        "            K.ColorJitter(hue=0.01, saturation=0.01, p=0.7))\n",
        "        self.noise_fac = 0.1\n",
        " \n",
        " \n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
        "        if self.noise_fac:\n",
        "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "            batch = batch + facs * torch.randn_like(batch)\n",
        "        return batch\n",
        " \n",
        " \n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        " \n",
        " \n",
        "def resize_image(image, out_size):\n",
        "    ratio = image.size[0] / image.size[1]\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "    return image.resize(size, Image.LANCZOS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "58987da6",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "58987da6",
        "outputId": "6797a856-ec57-4c7b-dab7-7f7795723126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "key_phrase :  ['Earth', 'moon', 'sun', 'Sea', 'sky']\n"
          ]
        }
      ],
      "source": [
        "#@title キーフレーズの入力・英訳\n",
        "#@markdown 入力例：\"地球\",\"月\",\"太陽\",\"海\",\"sky\"\n",
        "\n",
        "key_phrase = [\"地球\",\"月\",\"太陽\",\"海\",\"sky\"] #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "from googletrans import Translator\n",
        "tr = Translator(service_urls=['translate.googleapis.com'])\n",
        "\n",
        "en_key_phrase_list = []\n",
        "for src in key_phrase:\n",
        "    while True:\n",
        "        try:\n",
        "            text = tr.translate(src, dest=\"en\").text\n",
        "            break\n",
        "        except Exception as e:\n",
        "            tr = Translator(service_urls=['translate.googleapis.com'])\n",
        "    en_key_phrase_list.append(text)\n",
        "\n",
        "key_phrase_list = en_key_phrase_list\n",
        "print(\"key_phrase : \",en_key_phrase_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "cc757f55",
      "metadata": {
        "cellView": "form",
        "id": "cc757f55"
      },
      "outputs": [],
      "source": [
        "#@title スタイルの選択\n",
        "\n",
        "import random\n",
        "\n",
        "art_style = 'in vray unreal engine hyperrealistic' #@param [' in vray unreal engine hyperrealistic', ' in Ghibli style', ' in Raphael style', ' in banksy style', ' in Makoto Shinkai style','your choice!']\n",
        "\n",
        "#@markdown ”your choice!”を選択した場合には以下に任意のスタイルを英語で入力\n",
        "if art_style == 'your choice!':\n",
        "    your_style = \"\" #@param {type:\"string\"}\n",
        "    art_style = your_style\n",
        "\n",
        "#@markdown tにはアニメーションの長さを”秒”で入力\n",
        "t = 30 #@param {type:\"number\"}\n",
        "w = int(t*6)\n",
        "\n",
        "#text_prompts\n",
        "text_prompts_list = [\"'\",key_phrase_list[3],\" \",art_style,\"'\",':{0: 0, ',str(t),': 1}, ',\n",
        "                     \"'\",key_phrase_list[1],\" \",art_style,\"'\",':{',str(t),': 0 ',\",\", str(t*2),': 1}, ',\n",
        "                     \"'\",key_phrase_list[0],\" \",art_style,\"'\",':{',str(t*2),': 0 ',\",\", str(t*3),': 1}, ',\n",
        "                     \"'\",key_phrase_list[4],\" \",art_style,\"'\",':{',str(t*3),': 0 ',\",\", str(t*4),': 1}, ',\n",
        "                     \"'\",key_phrase_list[2],\" \",art_style,\"'\",':{',str(t*4),': 0 ',\",\", str(t*5),': 1}']\n",
        "\n",
        "tmp_text_prompts = \"\".join(text_prompts_list)\n",
        "\n",
        "#angle\n",
        "angles = random.sample([15,30,45,90,180,360], 5)\n",
        "\n",
        "#tmp_angle_list = ['0',':0, ',str(int(t/2)),':',str(angles[0]),', ',str(t),':', str(-angles[0]),', ',\n",
        "#                  str(int(t+t/2)),':',str(angles[1]),', ',str(t*2),':', str(-angles[1]),', ',\n",
        "#                  str(int(t*2+t/2)),':',str(angles[2]),', ',str(t*3),':', str(-angles[2]),', ',\n",
        "#                  str(int(t*3+t/2)),':',str(angles[3]),', ', str(t*4),':', str(-angles[3]),', ',\n",
        "#                  str(int(t*4+t/2)),':',str(angles[4]),', ', str(t*5),':', str(-angles[4])]\n",
        "\n",
        "tmp_angle_list = ['0',':0, ',str(int(t/2)),':',str(angles[0]),', ',str(t),':', str(-angles[0]),', ',\n",
        "                  str(int(t*2+t/2)),':',str(angles[2]),', ',str(t*3),':', str(-angles[2]),', ',\n",
        "                  str(int(t*4+t/2)),':',str(angles[4]),', ', str(t*5),':', str(-angles[4])]\n",
        "\n",
        "tmp_angle = \"\".join(tmp_angle_list)\n",
        "\n",
        "#zoom\n",
        "zoom_in = random.sample([1,1.25,1.5,1.75,2], 5)\n",
        "zoom_out = random.sample([0.5,0.625,0.75,0.875,1], 5)\n",
        "\n",
        "#tmp_zoom_list = ['0',':1, ',str(int(t/2)),':',str(zoom_in[0]),', ',str(t),':', str(zoom_out[0]),', ',\n",
        "#                 str(int(t+t/2)),':',str(zoom_in[1]),', ',str(t*2),':', str(zoom_out[1]),', ',\n",
        "#                 str(int(t*2+t/2)),':',str(zoom_in[2]),', ',str(t*3),':', str(zoom_out[2]),', ',\n",
        "#                 str(int(t*3+t/2)),':',str(zoom_in[3]),', ', str(t*4),':', str(zoom_out[3]),', ',\n",
        "#                 str(int(t*4+t/2)),':',str(zoom_in[4]),', ', str(t*5),':', str(zoom_out[4])]\n",
        "\n",
        "tmp_zoom_list = ['0',':1, ',str(int(t/2)),':',str(zoom_in[0]),', ',str(t),':', str(zoom_out[0]),', ',\n",
        "                 str(int(t*2+t/2)),':',str(zoom_in[2]),', ',str(t*3),':', str(zoom_out[2]),', ',\n",
        "                 str(int(t*4+t/2)),':',str(zoom_in[4]),', ', str(t*5),':', str(zoom_out[4])]\n",
        "\n",
        "\n",
        "tmp_zoom = \"\".join(tmp_zoom_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c3f3306",
      "metadata": {
        "scrolled": false,
        "cellView": "form",
        "id": "7c3f3306"
      },
      "outputs": [],
      "source": [
        "#@title パラメータ設定\n",
        "\n",
        "key_frames = True\n",
        "text_prompts = tmp_text_prompts\n",
        "width =  400\n",
        "height =  400\n",
        "model = \"vqgan_imagenet_f16_16384\"\n",
        "interval =  1\n",
        "\n",
        "src_img = Image.open(\"/content/text2animation/blank_img.jpg\")\n",
        "src_img = src_img.resize((src_img.width // 2, src_img.height // 2))\n",
        "src_img.save(\"/content/text2animation/blank_img.jpg\")\n",
        "src_img = \"/content/text2animation/blank_img.jpg\"\n",
        "\n",
        "initial_image = src_img\n",
        "target_images_list = [\"'\",src_img,\"'\",':{0: 0, ',str(w),': 1}']\n",
        "target_images = \"\".join(target_images_list)\n",
        "#target_images = \"\"\n",
        "seed = -1\n",
        "max_frames = w\n",
        "angle = tmp_angle\n",
        "zoom = tmp_zoom\n",
        "translation_x = \"0: 0\"\n",
        "translation_y = \"0: 0\"\n",
        "iterations_per_frame = \"0: 10\"\n",
        "save_all_iterations = False\n",
        "\n",
        "# option -C - skips download if already exists\n",
        "!curl -C - -L -o {model}.yaml -C - 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #ImageNet 1024\n",
        "!curl -C - -L -o {model}.ckpt -C - 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fckpts%2Flast.ckpt&dl=1'  #ImageNet 1024\n",
        "\n",
        "if initial_image != \"\":\n",
        "    print(\n",
        "        \"WARNING: You have specified an initial image. Note that the image resolution \"\n",
        "        \"will be inherited from this image, not whatever width and height you specified. \"\n",
        "        \"If the initial image resolution is too high, this can result in out of memory errors.\"\n",
        "    )\n",
        "elif width * height > 160000:\n",
        "    print(\n",
        "        \"WARNING: The width and height you have specified may be too high, in which case \"\n",
        "        \"you will encounter out of memory errors either at the image generation stage or the \"\n",
        "        \"video synthesis stage. If so, try reducing the resolution\"\n",
        "    )\n",
        "model_names={\n",
        "    \"vqgan_imagenet_f16_16384\": 'ImageNet 16384',\n",
        "    \"vqgan_imagenet_f16_1024\":\"ImageNet 1024\", \n",
        "    \"wikiart_1024\":\"WikiArt 1024\",\n",
        "    \"wikiart_16384\":\"WikiArt 16384\",\n",
        "    \"coco\":\"COCO-Stuff\",\n",
        "    \"faceshq\":\"FacesHQ\",\n",
        "    \"sflckr\":\"S-FLCKR\"\n",
        "}\n",
        "model_name = model_names[model]\n",
        "\n",
        "if seed == -1:\n",
        "    seed = None\n",
        "\n",
        "def parse_key_frames(string, prompt_parser=None):\n",
        "    \"\"\"Given a string representing frame numbers paired with parameter values at that frame,\n",
        "    return a dictionary with the frame numbers as keys and the parameter values as the values.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    string: string\n",
        "        Frame numbers paired with parameter values at that frame number, in the format\n",
        "        'framenumber1: (parametervalues1), framenumber2: (parametervalues2), ...'\n",
        "    prompt_parser: function or None, optional\n",
        "        If provided, prompt_parser will be applied to each string of parameter values.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Frame numbers as keys, parameter values at that frame number as values\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    RuntimeError\n",
        "        If the input string does not match the expected format.\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> parse_key_frames(\"10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\")\n",
        "    {10: 'Apple: 1| Orange: 0', 20: 'Apple: 0| Orange: 1| Peach: 1'}\n",
        "\n",
        "    >>> parse_key_frames(\"10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\", prompt_parser=lambda x: x.lower()))\n",
        "    {10: 'apple: 1| orange: 0', 20: 'apple: 0| orange: 1| peach: 1'}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # This is the preferred way, the regex way will eventually be deprecated.\n",
        "        frames = ast.literal_eval('{' + string + '}')\n",
        "        if isinstance(frames, set):\n",
        "            # If user forgot keyframes, just set value of frame 0\n",
        "            (frame,) = list(frames)\n",
        "            frames = {0: frame}\n",
        "        return frames\n",
        "    except Exception:\n",
        "        import re\n",
        "        pattern = r'((?P<frame>[0-9]+):[\\s]*[\\(](?P<param>[\\S\\s]*?)[\\)])'\n",
        "        frames = dict()\n",
        "        for match_object in re.finditer(pattern, string):\n",
        "            frame = int(match_object.groupdict()['frame'])\n",
        "            param = match_object.groupdict()['param']\n",
        "            if prompt_parser:\n",
        "                frames[frame] = prompt_parser(param)\n",
        "            else:\n",
        "                frames[frame] = param\n",
        "\n",
        "        if frames == {} and len(string) != 0:\n",
        "            raise RuntimeError(f'Key Frame string not correctly formatted: {string}')\n",
        "        return frames\n",
        "\n",
        "# Defaults, if left empty\n",
        "if angle == \"\":\n",
        "    angle = \"0\"\n",
        "if zoom == \"\":\n",
        "    zoom = \"1\"\n",
        "if translation_x == \"\":\n",
        "    translation_x = \"0\"\n",
        "if translation_y == \"\":\n",
        "    translation_y = \"0\"\n",
        "if iterations_per_frame == \"\":\n",
        "    iterations_per_frame = \"10\"\n",
        "\n",
        "if key_frames:\n",
        "    parameter_dicts = dict()\n",
        "    parameter_dicts['zoom'] = parse_key_frames(zoom, prompt_parser=float)\n",
        "    parameter_dicts['angle'] = parse_key_frames(angle, prompt_parser=float)\n",
        "    parameter_dicts['translation_x'] = parse_key_frames(translation_x, prompt_parser=float)\n",
        "    parameter_dicts['translation_y'] = parse_key_frames(translation_y, prompt_parser=float)\n",
        "    parameter_dicts['iterations_per_frame'] = parse_key_frames(iterations_per_frame, prompt_parser=int)\n",
        "\n",
        "    text_prompts_dict = parse_key_frames(text_prompts)\n",
        "    if all([isinstance(value, dict) for value in list(text_prompts_dict.values())]):\n",
        "       for key, value in list(text_prompts_dict.items()):\n",
        "           parameter_dicts[f'text_prompt: {key}'] = value\n",
        "    else:\n",
        "        # Old format\n",
        "        text_prompts_dict = parse_key_frames(text_prompts, prompt_parser=lambda x: x.split('|'))\n",
        "        for frame, prompt_list in text_prompts_dict.items():\n",
        "            for prompt in prompt_list:\n",
        "                prompt_key, prompt_value = prompt.split(\":\")\n",
        "                prompt_key = f'text_prompt: {prompt_key.strip()}'\n",
        "                prompt_value = prompt_value.strip()\n",
        "                if prompt_key not in parameter_dicts:\n",
        "                    parameter_dicts[prompt_key] = dict()\n",
        "                parameter_dicts[prompt_key][frame] = prompt_value\n",
        "\n",
        "\n",
        "    image_prompts_dict = parse_key_frames(target_images)\n",
        "    if all([isinstance(value, dict) for value in list(image_prompts_dict.values())]):\n",
        "        for key, value in list(image_prompts_dict.items()):\n",
        "           parameter_dicts[f'image_prompt: {key}'] = value\n",
        "    else:\n",
        "        # Old format\n",
        "        image_prompts_dict = parse_key_frames(target_images, prompt_parser=lambda x: x.split('|'))\n",
        "        for frame, prompt_list in image_prompts_dict.items():\n",
        "            for prompt in prompt_list:\n",
        "                prompt_key, prompt_value = prompt.split(\":\")\n",
        "                prompt_key = f'image_prompt: {prompt_key.strip()}'\n",
        "                prompt_value = prompt_value.strip()\n",
        "                if prompt_key not in parameter_dicts:\n",
        "                    parameter_dicts[prompt_key] = dict()\n",
        "                parameter_dicts[prompt_key][frame] = prompt_value\n",
        "\n",
        "\n",
        "def add_inbetweens():\n",
        "    global text_prompts\n",
        "    global target_images\n",
        "    global zoom\n",
        "    global angle\n",
        "    global translation_x\n",
        "    global translation_y\n",
        "    global iterations_per_frame\n",
        "\n",
        "    global text_prompts_series\n",
        "    global target_images_series\n",
        "    global zoom_series\n",
        "    global angle_series\n",
        "    global translation_x_series\n",
        "    global translation_y_series\n",
        "    global iterations_per_frame_series\n",
        "    global model\n",
        "    global args\n",
        "    def get_inbetweens(key_frames_dict, integer=False):\n",
        "        \"\"\"Given a dict with frame numbers as keys and a parameter value as values,\n",
        "        return a pandas Series containing the value of the parameter at every frame from 0 to max_frames.\n",
        "        Any values not provided in the input dict are calculated by linear interpolation between\n",
        "        the values of the previous and next provided frames. If there is no previous provided frame, then\n",
        "        the value is equal to the value of the next provided frame, or if there is no next provided frame,\n",
        "        then the value is equal to the value of the previous provided frame. If no frames are provided,\n",
        "        all frame values are NaN.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        key_frames_dict: dict\n",
        "            A dict with integer frame numbers as keys and numerical values of a particular parameter as values.\n",
        "        integer: Bool, optional\n",
        "            If True, the values of the output series are converted to integers.\n",
        "            Otherwise, the values are floats.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        pd.Series\n",
        "            A Series with length max_frames representing the parameter values for each frame.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> max_frames = 5\n",
        "        >>> get_inbetweens({1: 5, 3: 6})\n",
        "        0    5.0\n",
        "        1    5.0\n",
        "        2    5.5\n",
        "        3    6.0\n",
        "        4    6.0\n",
        "        dtype: float64\n",
        "\n",
        "        >>> get_inbetweens({1: 5, 3: 6}, integer=True)\n",
        "        0    5\n",
        "        1    5\n",
        "        2    5\n",
        "        3    6\n",
        "        4    6\n",
        "        dtype: int64\n",
        "        \"\"\"\n",
        "        key_frame_series = pd.Series([np.nan for a in range(max_frames)])\n",
        "        for i, value in key_frames_dict.items():\n",
        "            key_frame_series[i] = value\n",
        "        key_frame_series = key_frame_series.astype(float)\n",
        "        key_frame_series = key_frame_series.interpolate(limit_direction='both')\n",
        "        if integer:\n",
        "            return key_frame_series.astype(int)\n",
        "        return key_frame_series\n",
        "\n",
        "    if key_frames:\n",
        "        text_prompts_series_dict = dict()\n",
        "        for parameter in parameter_dicts.keys():\n",
        "            if len(parameter_dicts[parameter]) > 0:\n",
        "                if parameter.startswith('text_prompt:'):\n",
        "                    try:\n",
        "                        text_prompts_series_dict[parameter] = get_inbetweens(parameter_dicts[parameter])\n",
        "                    except RuntimeError as e:\n",
        "                        raise RuntimeError(\n",
        "                            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "                            \"formatted `text_prompts` correctly for key frames.\\n\"\n",
        "                            \"Please read the instructions to find out how to use key frames \"\n",
        "                            \"correctly.\\n\"\n",
        "                        )\n",
        "        text_prompts_series = pd.Series([np.nan for a in range(max_frames)])\n",
        "        for i in range(max_frames):\n",
        "            combined_prompt = []\n",
        "            for parameter, value in text_prompts_series_dict.items():\n",
        "                parameter = parameter[len('text_prompt:'):].strip()\n",
        "                combined_prompt.append(f'{parameter}: {value[i]}')\n",
        "            text_prompts_series[i] = ' | '.join(combined_prompt)\n",
        "\n",
        "        image_prompts_series_dict = dict()\n",
        "        for parameter in parameter_dicts.keys():\n",
        "            if len(parameter_dicts[parameter]) > 0:\n",
        "                if parameter.startswith('image_prompt:'):\n",
        "                    try:\n",
        "                        image_prompts_series_dict[parameter] = get_inbetweens(parameter_dicts[parameter])\n",
        "                    except RuntimeError as e:\n",
        "                        raise RuntimeError(\n",
        "                            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "                            \"formatted `image_prompts` correctly for key frames.\\n\"\n",
        "                            \"Please read the instructions to find out how to use key frames \"\n",
        "                            \"correctly.\\n\"\n",
        "                        )\n",
        "        target_images_series = pd.Series([np.nan for a in range(max_frames)])\n",
        "        for i in range(max_frames):\n",
        "            combined_prompt = []\n",
        "            for parameter, value in image_prompts_series_dict.items():\n",
        "                parameter = parameter[len('image_prompt:'):].strip()\n",
        "                combined_prompt.append(f'{parameter}: {value[i]}')\n",
        "            target_images_series[i] = ' | '.join(combined_prompt)\n",
        "\n",
        "        try:\n",
        "            angle_series = get_inbetweens(parameter_dicts['angle'])\n",
        "        except RuntimeError as e:\n",
        "            print(\n",
        "                \"WARNING: You have selected to use key frames, but you have not \"\n",
        "                \"formatted `angle` correctly for key frames.\\n\"\n",
        "                \"Attempting to interpret `angle` as \"\n",
        "                f'\"0: ({angle})\"\\n'\n",
        "                \"Please read the instructions to find out how to use key frames \"\n",
        "                \"correctly.\\n\"\n",
        "            )\n",
        "            angle = f\"0: ({angle})\"\n",
        "            angle_series = get_inbetweens(parse_key_frames(angle))\n",
        "\n",
        "        try:\n",
        "            zoom_series = get_inbetweens(parameter_dicts['zoom'])\n",
        "        except RuntimeError as e:\n",
        "            print(\n",
        "                \"WARNING: You have selected to use key frames, but you have not \"\n",
        "                \"formatted `zoom` correctly for key frames.\\n\"\n",
        "                \"Attempting to interpret `zoom` as \"\n",
        "                f'\"0: ({zoom})\"\\n'\n",
        "                \"Please read the instructions to find out how to use key frames \"\n",
        "                \"correctly.\\n\"\n",
        "            )\n",
        "            zoom = f\"0: ({zoom})\"\n",
        "            zoom_series = get_inbetweens(parse_key_frames(zoom))\n",
        "        for i, zoom in enumerate(zoom_series):\n",
        "            if zoom <= 0:\n",
        "                print(\n",
        "                    f\"WARNING: You have selected a zoom of {zoom} at frame {i}. \"\n",
        "                    \"This is meaningless. \"\n",
        "                    \"If you want to zoom out, use a value between 0 and 1. \"\n",
        "                    \"If you want no zoom, use a value of 1.\"\n",
        "                )\n",
        "\n",
        "        try:\n",
        "            translation_x_series = get_inbetweens(parameter_dicts['translation_x'])\n",
        "        except RuntimeError as e:\n",
        "            print(\n",
        "                \"WARNING: You have selected to use key frames, but you have not \"\n",
        "                \"formatted `translation_x` correctly for key frames.\\n\"\n",
        "                \"Attempting to interpret `translation_x` as \"\n",
        "                f'\"0: ({translation_x})\"\\n'\n",
        "                \"Please read the instructions to find out how to use key frames \"\n",
        "                \"correctly.\\n\"\n",
        "            )\n",
        "            translation_x = f\"0: ({translation_x})\"\n",
        "            translation_x_series = get_inbetweens(parse_key_frames(translation_x))\n",
        "\n",
        "        try:\n",
        "            translation_y_series = get_inbetweens(parameter_dicts['translation_y'])\n",
        "        except RuntimeError as e:\n",
        "            print(\n",
        "                \"WARNING: You have selected to use key frames, but you have not \"\n",
        "                \"formatted `translation_y` correctly for key frames.\\n\"\n",
        "                \"Attempting to interpret `translation_y` as \"\n",
        "                f'\"0: ({translation_y})\"\\n'\n",
        "                \"Please read the instructions to find out how to use key frames \"\n",
        "                \"correctly.\\n\"\n",
        "            )\n",
        "            translation_y = f\"0: ({translation_y})\"\n",
        "            translation_y_series = get_inbetweens(parse_key_frames(translation_y))\n",
        "\n",
        "        try:\n",
        "            iterations_per_frame_series = get_inbetweens(\n",
        "                parameter_dicts['iterations_per_frame'], integer=True\n",
        "            )\n",
        "        except RuntimeError as e:\n",
        "            print(\n",
        "                \"WARNING: You have selected to use key frames, but you have not \"\n",
        "                \"formatted `iterations_per_frame` correctly for key frames.\\n\"\n",
        "                \"Attempting to interpret `iterations_per_frame` as \"\n",
        "                f'\"0: ({iterations_per_frame})\"\\n'\n",
        "                \"Please read the instructions to find out how to use key frames \"\n",
        "                \"correctly.\\n\"\n",
        "            )\n",
        "            iterations_per_frame = f\"0: ({iterations_per_frame})\"\n",
        "            \n",
        "            iterations_per_frame_series = get_inbetweens(\n",
        "                parse_key_frames(iterations_per_frame), integer=True\n",
        "            )\n",
        "    else:\n",
        "        text_prompts = [phrase.strip() for phrase in text_prompts.split(\"|\")]\n",
        "        if text_prompts == ['']:\n",
        "            text_prompts = []\n",
        "        if target_images == \"None\" or not target_images:\n",
        "            target_images = []\n",
        "        else:\n",
        "            target_images = target_images.split(\"|\")\n",
        "            target_images = [image.strip() for image in target_images]\n",
        "\n",
        "        angle = float(angle)\n",
        "        zoom = float(zoom)\n",
        "        translation_x = float(translation_x)\n",
        "        translation_y = float(translation_y)\n",
        "        iterations_per_frame = int(iterations_per_frame)\n",
        "        if zoom <= 0:\n",
        "            print(\n",
        "                f\"WARNING: You have selected a zoom of {zoom}. \"\n",
        "                \"This is meaningless. \"\n",
        "                \"If you want to zoom out, use a value between 0 and 1. \"\n",
        "                \"If you want no zoom, use a value of 1.\"\n",
        "            )\n",
        "\n",
        "    args = argparse.Namespace(\n",
        "        prompts=text_prompts,\n",
        "        image_prompts=target_images,\n",
        "        noise_prompt_seeds=[],\n",
        "        noise_prompt_weights=[],\n",
        "        size=[width, height],\n",
        "        init_weight=0.,\n",
        "        clip_model='ViT-B/32',\n",
        "        vqgan_config=f'{model}.yaml',\n",
        "        vqgan_checkpoint=f'{model}.ckpt',\n",
        "        step_size=0.1,\n",
        "        cutn=64,\n",
        "        cut_pow=1.,\n",
        "        display_freq=interval,\n",
        "        seed=seed,\n",
        "    )\n",
        "\n",
        "add_inbetweens()\n",
        "\n",
        "print(\"done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fbfa978",
      "metadata": {
        "scrolled": true,
        "cellView": "form",
        "id": "8fbfa978"
      },
      "outputs": [],
      "source": [
        "#@title 画像生成\n",
        "\n",
        "path = f'{working_dir}/steps/'\n",
        "shutil.rmtree(path)\n",
        "os.mkdir(path)\n",
        "print(path)\n",
        "\n",
        "# Delete memory from previous runs\n",
        "!nvidia-smi -caa\n",
        "for var in ['device', 'model', 'perceptor', 'z']:\n",
        "  try:\n",
        "      del globals()[var]\n",
        "  except:\n",
        "      pass\n",
        "\n",
        "try:\n",
        "    import gc\n",
        "    gc.collect()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    torch.cuda.empty_cache()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "if not key_frames:\n",
        "    if text_prompts:\n",
        "        print('Using text prompts:', text_prompts)\n",
        "    if target_images:\n",
        "        print('Using image prompts:', target_images)\n",
        "if args.seed is None:\n",
        "    seed = torch.seed()\n",
        "else:\n",
        "    seed = args.seed\n",
        "torch.manual_seed(seed)\n",
        "print('Using seed:', seed)\n",
        " \n",
        "model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
        "perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        " \n",
        "cut_size = perceptor.visual.input_resolution\n",
        "e_dim = model.quantize.e_dim\n",
        "f = 2**(model.decoder.num_resolutions - 1)\n",
        "make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n",
        "n_toks = model.quantize.n_e\n",
        "toksX, toksY = args.size[0] // f, args.size[1] // f\n",
        "sideX, sideY = toksX * f, toksY * f\n",
        "z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "stop_on_next_loop = False  # Make sure GPU memory doesn't get corrupted from cancelling the run mid-way through, allow a full frame to complete\n",
        "\n",
        "def read_image_workaround(path):\n",
        "    \"\"\"OpenCV reads images as BGR, Pillow saves them as RGB. Work around\n",
        "    this incompatibility to avoid colour inversions.\"\"\"\n",
        "    im_tmp = cv2.imread(path)\n",
        "    return cv2.cvtColor(im_tmp, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "for i in range(max_frames):\n",
        "    if stop_on_next_loop:\n",
        "      break\n",
        "    if key_frames:\n",
        "        text_prompts = text_prompts_series[i]\n",
        "        text_prompts = [phrase.strip() for phrase in text_prompts.split(\"|\")]\n",
        "        if text_prompts == ['']:\n",
        "            text_prompts = []\n",
        "        args.prompts = text_prompts\n",
        "\n",
        "        target_images = target_images_series[i]\n",
        "\n",
        "        if target_images == \"None\" or not target_images:\n",
        "            target_images = []\n",
        "        else:\n",
        "            target_images = target_images.split(\"|\")\n",
        "            target_images = [image.strip() for image in target_images]\n",
        "        args.image_prompts = target_images\n",
        "\n",
        "        angle = angle_series[i]\n",
        "        zoom = zoom_series[i]\n",
        "        translation_x = translation_x_series[i]\n",
        "        translation_y = translation_y_series[i]\n",
        "        iterations_per_frame = iterations_per_frame_series[i]\n",
        "        print(\n",
        "            f'text_prompts: {text_prompts}',\n",
        "            f'image_prompts: {target_images}',\n",
        "            f'angle: {angle}',\n",
        "            f'zoom: {zoom}',\n",
        "            f'translation_x: {translation_x}',\n",
        "            f'translation_y: {translation_y}',\n",
        "            f'iterations_per_frame: {iterations_per_frame}'\n",
        "        )\n",
        "    try:\n",
        "        if i == 0 and initial_image != \"\":\n",
        "            img_0 = read_image_workaround(initial_image)\n",
        "            z, *_ = model.encode(TF.to_tensor(img_0).to(device).unsqueeze(0) * 2 - 1)\n",
        "        elif i == 0 and not os.path.isfile(f'{working_dir}/steps/{i:04d}.png'):\n",
        "            one_hot = F.one_hot(\n",
        "                torch.randint(n_toks, [toksY * toksX], device=device), n_toks\n",
        "            ).float()\n",
        "            z = one_hot @ model.quantize.embedding.weight\n",
        "            z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
        "        else:\n",
        "            if save_all_iterations:\n",
        "                img_0 = read_image_workaround(\n",
        "                    f'{working_dir}/steps/{i:04d}_{iterations_per_frame}.png')\n",
        "            else:\n",
        "                img_0 = read_image_workaround(f'{working_dir}/steps/{i:04d}.png')\n",
        "\n",
        "            center = (1*img_0.shape[1]//2, 1*img_0.shape[0]//2)\n",
        "            trans_mat = np.float32(\n",
        "                [[1, 0, translation_x],\n",
        "                [0, 1, translation_y]]\n",
        "            )\n",
        "            rot_mat = cv2.getRotationMatrix2D( center, angle, zoom )\n",
        "\n",
        "            trans_mat = np.vstack([trans_mat, [0,0,1]])\n",
        "            rot_mat = np.vstack([rot_mat, [0,0,1]])\n",
        "            transformation_matrix = np.matmul(rot_mat, trans_mat)\n",
        "\n",
        "            img_0 = cv2.warpPerspective(\n",
        "                img_0,\n",
        "                transformation_matrix,\n",
        "                (img_0.shape[1], img_0.shape[0]),\n",
        "                borderMode=cv2.BORDER_WRAP\n",
        "            )\n",
        "            z, *_ = model.encode(TF.to_tensor(img_0).to(device).unsqueeze(0) * 2 - 1)\n",
        "        i += 1\n",
        "\n",
        "        z_orig = z.clone()\n",
        "        z.requires_grad_(True)\n",
        "        opt = optim.Adam([z], lr=args.step_size)\n",
        "\n",
        "        normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                        std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "        pMs = []\n",
        "\n",
        "        for prompt in args.prompts:\n",
        "            txt, weight, stop = parse_prompt(prompt)\n",
        "            embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "            pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "        for prompt in args.image_prompts:\n",
        "            path, weight, stop = parse_prompt(prompt)\n",
        "            img = resize_image(Image.open(path).convert('RGB'), (sideX, sideY))\n",
        "            batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "            embed = perceptor.encode_image(normalize(batch)).float()\n",
        "            pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "        for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
        "            gen = torch.Generator().manual_seed(seed)\n",
        "            embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "            pMs.append(Prompt(embed, weight).to(device))\n",
        "\n",
        "        def synth(z):\n",
        "            z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "            return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "        def add_xmp_data(filename):\n",
        "            imagen = ImgTag(filename=filename)\n",
        "            imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'creator', 'VQGAN+CLIP', {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "            if args.prompts:\n",
        "                imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'title', \" | \".join(args.prompts), {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "            else:\n",
        "                imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'title', 'None', {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "            imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'i', str(i), {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "            imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'model', model_name, {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "            imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'seed',str(seed) , {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "            imagen.close()\n",
        "\n",
        "        def add_stegano_data(filename):\n",
        "            data = {\n",
        "                \"title\": \" | \".join(args.prompts) if args.prompts else None,\n",
        "                \"notebook\": \"VQGAN+CLIP\",\n",
        "                \"i\": i,\n",
        "                \"model\": model_name,\n",
        "                \"seed\": str(seed),\n",
        "            }\n",
        "            lsb.hide(filename, json.dumps(data)).save(filename)\n",
        "\n",
        "        @torch.no_grad()\n",
        "        def checkin(i, losses):\n",
        "            losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "            tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "            out = synth(z)\n",
        "            TF.to_pil_image(out[0].cpu()).save('progress.png')\n",
        "            add_stegano_data('progress.png')\n",
        "            add_xmp_data('progress.png')\n",
        "            display.display(display.Image('progress.png'))\n",
        "\n",
        "        def save_output(i, img, suffix=None):\n",
        "            filename = \\\n",
        "                f\"{working_dir}/steps/{i:04}{'_' + suffix if suffix else ''}.png\"\n",
        "            imageio.imwrite(filename, np.array(img))\n",
        "            add_stegano_data(filename)\n",
        "            add_xmp_data(filename)\n",
        "\n",
        "        def ascend_txt(i, save=True, suffix=None):\n",
        "            out = synth(z)\n",
        "            iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n",
        "\n",
        "            result = []\n",
        "\n",
        "            if args.init_weight:\n",
        "                result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)\n",
        "\n",
        "            for prompt in pMs:\n",
        "                result.append(prompt(iii))\n",
        "            img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n",
        "            img = np.transpose(img, (1, 2, 0))\n",
        "            if save:\n",
        "                save_output(i, img, suffix=suffix)\n",
        "            return result\n",
        "\n",
        "        def train(i, save=True, suffix=None):\n",
        "            opt.zero_grad()\n",
        "            lossAll = ascend_txt(i, save=save, suffix=suffix)\n",
        "            if i % args.display_freq == 0 and save:\n",
        "                checkin(i, lossAll)\n",
        "            loss = sum(lossAll)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            with torch.no_grad():\n",
        "                z.copy_(z.maximum(z_min).minimum(z_max))\n",
        "\n",
        "        with tqdm() as pbar:\n",
        "            if iterations_per_frame == 0:\n",
        "                save_output(i, img_0)\n",
        "            j = 1\n",
        "            while True:\n",
        "                suffix = (str(j) if save_all_iterations else None)\n",
        "                if j >= iterations_per_frame:\n",
        "                    train(i, save=True, suffix=suffix)\n",
        "                    break\n",
        "                if save_all_iterations:\n",
        "                    train(i, save=True, suffix=suffix)\n",
        "                else:\n",
        "                    train(i, save=False, suffix=suffix)\n",
        "                j += 1\n",
        "                pbar.update()\n",
        "    except KeyboardInterrupt:\n",
        "      stop_on_next_loop = True\n",
        "      pass\n",
        "\n",
        "print(\"done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18555e2f",
      "metadata": {
        "scrolled": true,
        "cellView": "form",
        "id": "18555e2f"
      },
      "outputs": [],
      "source": [
        "#@title SRCNNによる超解像(Option)\n",
        "\n",
        "!git clone https://github.com/Mirwaisse/SRCNN.git\n",
        "!curl https://raw.githubusercontent.com/chigozienri/SRCNN/master/models/model_2x.pth -o model_2x.pth\n",
        "\n",
        "# import subprocess in case this cell is run without the above cells\n",
        "import subprocess\n",
        "# Set zoomed = True if this cell is run\n",
        "zoomed = True\n",
        "\n",
        "init_frame = 1\n",
        "last_frame = w\n",
        "\n",
        "for i in range(init_frame, last_frame + 1): #\n",
        "    filename = f\"{i:04}.png\"\n",
        "    cmd = [\n",
        "        'python',\n",
        "        '/content/SRCNN/run.py',\n",
        "        '--zoom_factor',\n",
        "        '2',  # Note if you increase this, you also need to change the model.\n",
        "        '--model',\n",
        "        '/content/model_2x.pth',  # 2x, 3x and 4x are available from the repo above\n",
        "        '--image',\n",
        "        filename,\n",
        "        '--cuda'\n",
        "    ]\n",
        "    print(f'Upscaling frame {i}')\n",
        "\n",
        "    process = subprocess.Popen(cmd, cwd=f'{working_dir}/steps/')\n",
        "    stdout, stderr = process.communicate()\n",
        "    if process.returncode != 0:\n",
        "        print(stderr)\n",
        "        print(\n",
        "            \"You may be able to avoid this error by backing up the frames,\"\n",
        "            \"restarting the notebook, and running only the video synthesis cells,\"\n",
        "            \"or by decreasing the resolution of the image generation steps. \"\n",
        "            \"If you restart the notebook, you will have to define the `filepath` manually\"\n",
        "            \"by adding `filepath = 'PATH_TO_THE_VIDEO'` to the beginning of this cell. \"\n",
        "            \"If these steps do not work, please post the traceback in the github.\"\n",
        "        )\n",
        "        raise RuntimeError(stderr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bf60b23",
      "metadata": {
        "scrolled": false,
        "cellView": "form",
        "id": "6bf60b23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46247748-6a0a-4ce8-8f6c-94df1e9b8166"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The video is ready\n"
          ]
        }
      ],
      "source": [
        "#@title アニメーション作成\n",
        "\n",
        "# import subprocess in case this cell is run without the above cells\n",
        "import subprocess\n",
        "\n",
        "# Try to avoid OOM errors\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "init_frame = 1\n",
        "last_frame = w\n",
        "fps = 6\n",
        "\n",
        "try:\n",
        "    key_frames\n",
        "except NameError:\n",
        "    filename = \"video.mp4\"\n",
        "else:\n",
        "    if key_frames:\n",
        "        # key frame filename would be too long\n",
        "        filename = \"video.mp4\"\n",
        "    else:\n",
        "        filename = f\"{'_'.join(text_prompts).replace(' ', '')}.mp4\"\n",
        "filepath = f'{working_dir}/{filename}'\n",
        "\n",
        "frames = []\n",
        "# tqdm.write('Generating video...')\n",
        "try:\n",
        "    zoomed\n",
        "except NameError:\n",
        "    image_path = f'{working_dir}/steps/%04d.png'\n",
        "else:\n",
        "    image_path = f'{working_dir}/steps/zoomed_%04d.png'\n",
        "\n",
        "cmd = [\n",
        "    'ffmpeg',\n",
        "    '-y',\n",
        "    '-vcodec',\n",
        "    'png',\n",
        "    '-r',\n",
        "    str(fps),\n",
        "    '-start_number',\n",
        "    str(init_frame),\n",
        "    '-i',\n",
        "    image_path,\n",
        "    '-c:v',\n",
        "    'libx264',\n",
        "    '-frames:v',\n",
        "    str(last_frame-init_frame),\n",
        "    '-vf',\n",
        "    f'fps={fps}',\n",
        "    '-pix_fmt',\n",
        "    'yuv420p',\n",
        "    '-crf',\n",
        "    '17',\n",
        "    '-preset',\n",
        "    'veryslow',\n",
        "    filepath\n",
        "]\n",
        "\n",
        "process = subprocess.Popen(cmd, cwd=f'{working_dir}/steps/', stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "stdout, stderr = process.communicate()\n",
        "if process.returncode != 0:\n",
        "    print(stderr)\n",
        "    print(\n",
        "        \"You may be able to avoid this error by backing up the frames,\"\n",
        "        \"restarting the notebook, and running only the google drive/local connection and video synthesis cells,\"\n",
        "        \"or by decreasing the resolution of the image generation steps. \"\n",
        "        \"If these steps do not work, please post the traceback in the github.\"\n",
        "    )\n",
        "    raise RuntimeError(stderr)\n",
        "else:\n",
        "    print(\"The video is ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "936804fe",
      "metadata": {
        "cellView": "form",
        "id": "936804fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d5fad0d1-a671-43c8-d189-5c2f37b349c1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6511a980-a84d-4472-930e-9785130fc460\", \"video.mp4\", 466437)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Download as mp4\n",
        "from google.colab import files\n",
        "files.download(filepath)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "name": "text2animation.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}